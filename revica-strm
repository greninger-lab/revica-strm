#!/usr/bin/env python3

import argparse
import subprocess
import sys
from collections import defaultdict
import os
import signal

DIV_LEN = min(os.get_terminal_size()[0], 100)

def print_divider(color):
    print(f"\n{color}{"".join(["="] * DIV_LEN)}{CLR}\n")

def print_title(title, color):
    n = DIV_LEN - (len(title) + 2)
    n, d = n // 2, n % 2
    l, r = n + d, n
    print(f"\n{color}{"".join(l * ["="])} {title} {"".join(r * ["="])}{CLR}")

def header(s):
    return f"{B_GREEN}{s}{CLR}"

B_GREEN = "\033[1;32m"
B_YELLOW = "\033[1;33m"
B_RED = "\033[1;31m"
B_BLUE = "\033[1;34m"
B_MAGENTA = "\033[1;35m"
CLR = "\033[0m"
BROWN = "\033[0;33m"

ARG_TEXT=f"{B_MAGENTA}TEXT{CLR}"
ARG_PATH=f"{B_YELLOW}PATH{CLR}"
ARG_INT=f"{B_BLUE}INT{CLR}"
ARG_FLAG=f"{B_RED}FLAG{CLR}"
ARG_DECIMAL=f"{BROWN}DECIMAL{CLR}"

HELP = f"""
revica-strm - reference-based assembly of viral genomes

{header("USAGE:")}
revica-strm {B_BLUE}INPUT_DIR{CLR} (other options)

{header("Input/output:")}
{B_BLUE}INPUT_DIR{CLR}           {ARG_PATH}      Input path (should be a directory) with FASTQ/FASTQ.gz files
--output            {ARG_PATH}      Output directory (default: 'revica-output')
--run_name          {ARG_TEXT}      Name of the analysis run (default: 'run')
--refs              {ARG_PATH}      Reference FASTA file (default: uses pipeline's embedded assets/ref.fa)
--print_log         {ARG_FLAG}      Print pipeline output to screen
--unpaired_sra      {ARG_FLAG}      Include unpaired reads in SRA paired-end FASTQ files (creates single fastq of R1 and R2)

{header("Runtime")}
-profile            {ARG_TEXT}      Use with "docker" to run the pipeline with docker (avoids having to download all software used)
-c                  {ARG_PATH}      Path to nextflow .config file

{header("FASTP")}
--skip_fastp        {ARG_FLAG}      Skip fastp preprocessing step
--adapter_fasta     {ARG_PATH}      Custom adapter FASTA file (default: uses pipeline's assets/adapter.fasta)
--trim_len          {ARG_PATH}      Filter out reads below this length after trimming (default: 50)
--save_merged       {ARG_FLAG}      Save merged reads from fastp

{header("MultiQC")}
--skip_multiqc      {ARG_FLAG}      Skip MultiQC report generation

{header("Kraken2")}
--run_kraken2       {ARG_FLAG}      Use kraken2 to process only unclassified reads (should be paired with a human kraken2 database)
--kraken2_db        {ARG_PATH}      Path to Kraken2 database (default: uses pipeline's kraken2_db)

{header("Subsampling")}
--sample            {ARG_INT}       Use SEQTK to subsample reads to the number specified (default: off)
--save_sample_reads {ARG_FLAG}      Save the subsampled reads in the output directory.

{header("Reference selection thresholds")}
--ref_min_depth     {ARG_INT}       The minimum depth an alignment against a reference must have to qualify for assembly (default: 3)
--ref_min_cov       {ARG_INT}       The minimum coverage (in percent) an alignment against a reference to qualify for assembly (default: 30)

{header("Assembly construction thresholds")}
--cons_init_min_depth   {ARG_INT}   The minimum depth of an alignment against the initial consensus to continue construction (default: 5)
--cons_init_min_cov     {ARG_INT}   The minimum coverage of an alignment against the initial consensus to continue construction (default: 60)

--ivar_init_t       {ARG_DECIMAL}   The ambiguity threshold for iVar consensus when creating the initial consensus (default: 0.6)
--ivar_init_t       {ARG_INT}       The minimum mapping quality a read must have to be used during consensus construction (default: 20)
--var_init_m        {ARG_INT}       The minimum depth at an alignment base; if below this, base is marked as "N" (default: 1)

--ivar_fin_t        {ARG_DECIMAL}   The ambiguity threshold for iVar consensus when creating the final consensus (default: 0.4)
--ivar_fin_t        {ARG_INT}       The minimum mapping quality a read must have to be used during consensus construction (default: 20)
--var_fin_m         {ARG_INT}       The minimum depth at an alignment base; if below this, base is marked as "N" (default: 3)

{header("Compute resources")}
--max_cpus          {ARG_INT}       Maximum CPUs to use (default: 128)
--max_memory        {ARG_TEXT}      Maximum memory to use (default: '256.GB')
--max_time          {ARG_TEXT}      Maximum run time (default: '48.h')

{header("Misc")}
--branch            {ARG_TEXT}      The branch of the pipeline to use. Default (and stable): main. Unstable branch: staging.
--local             {ARG_FLAG}      Run the downloaded pipeline, expected to be in the current directory. Not recommended.
"""

def make_fastq_map(dir):
    files = [f for f in os.listdir(dir) if f.endswith(".fastq") or f.endswith(".fastq.gz")]

    paired_files = find_pairs(dir, files)
    return paired_files

def find_pairs(dir, files):
    pairs = defaultdict(list)
    for f in files:
        temp_delim = "." if "_" not in f else "_"
        pairs[f.split(temp_delim)[0]].append(os.path.join(dir, f))

    for pair in pairs.values():
        assert len(pair) <= 2, f"""
        more than 2 files per pair, file names are not unique enough: {pair};
        Ensure that filenames before the first '_' or '.' are unique to the sample.
        """
        pair.sort()
        if len(pair) == 1:
            pair.append("")

    return pairs

def confirm_samplesheet(fastq_map):
    print("\nsample\tfastq_1\tfastq_2")
    print("=======================================")
    for sample, row in fastq_map.items():
        r = [sample] + row
        print("\t".join(r))
    print("")

    while True:
            response = input("Does the samplesheet look correct? [y/n]").strip().lower()
            if response in ['yes', 'y']:
                return True
            elif response in ['no', 'n']:
                sys.exit(0)
            else:
                print("Please enter 'yes' or 'no'.")

def confirm_profile(profile_setting):
    if not profile_setting:
        r_text = "-profile is unset; this means that you'll run the pipeline locally, without using docker for dependencies. Is this what you want? [y/n]"
    elif profile_setting == "docker":
        r_text = "-profile is set to use docker; Is this what you want? [y/n]"
    else:
        r_text = "-profile is not set to anything recognizable: either set it to 'docker' or don't use it. Exiting..."
        exit(1)

    while True:
        response = input(r_text).strip().lower()
        if response in ['yes', 'y']:
            return True
        elif response in ['no', 'n']:
            sys.exit(0)
        else:
            print("Please enter 'yes' or 'no'")

def make_samplesheet(dir, samplesheet_name = "samplesheet.csv", sep = ","):
    fastq_map = make_fastq_map(dir)

    if sep == "," and not samplesheet_name.endswith(".csv"):
        samplesheet_name += ".csv"

    if sep == "\t" and not samplesheet_name.endswith(".tsv"):
        samplesheet_name += ".tsv"

    confirm_samplesheet(fastq_map)

    with open(samplesheet_name, "w") as outf:
        header = ["sample", "fastq_1", "fastq_2"]
        _ = outf.write(sep.join(header) + "\n")
        for sample, fastqs in fastq_map.items():
            _ = outf.write(f"{sample}{sep}{sep.join(fastqs)}\n")


def check_commit_history():
    output = subprocess.run(["nextflow", "pull", "greninger-lab/revica-strm"], stdout = subprocess.PIPE, stderr = subprocess.STDOUT)
    if "contains uncommitted changes" in str(output.stdout):
        print(output.stdout.decode("utf-8"))
        print(f"{B_RED}This is a routine error. To correct this, please run:{CLR}")
        print("rm -r $HOME/.nextflow/assets/greninger-lab/revica-strm/")
        print("And confirm deletion of all files (this clears nextflow's cache).")
        sys.exit(0)
    else:
        print(f"{B_GREEN}Commit history clean. Proceeding...{CLR}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(add_help=False, exit_on_error=False)
    _ = parser.add_argument("input", help = argparse.SUPPRESS)
    _ = parser.add_argument("-profile", required=False, default=None, help = argparse.SUPPRESS)
    _ = parser.add_argument("--print_log", action="store_true", help = argparse.SUPPRESS)
    _ = parser.add_argument("--branch", default="main", choices=["main", "staging"], help = argparse.SUPPRESS)
    _ = parser.add_argument("-h", "--help", action="store_true", help = argparse.SUPPRESS)
    _ = parser.add_argument("--local", action="store_true", help = argparse.SUPPRESS)

    try:
        args, extra = parser.parse_known_args()
    except argparse.ArgumentError as e:
        print(e)
        print(HELP)
        sys.exit(1)

    if args.help:
        print(HELP)
        sys.exit(0)

    profile = ["-profile", args.profile] if args.profile else []
    print_log = ["-process.echo"] if args.print_log else []

    extra = extra + profile + print_log

    print_title("RUN SETUP", B_YELLOW)
    make_samplesheet(args.input)
    confirm_profile(args.profile)
    print_divider(B_YELLOW)

    print_title("RUN LOAD", B_BLUE)
    if args.local:
        if not os.path.exists('main.nf'):
            print("Cannot find main.nf in current run directory (pipeline is not downloaded. Exiting...)")
            exit(1)

        print("Running pipeline locally...")
        run_cmd = ['nextflow', 'run', 'main.nf', '--input', 'samplesheet.csv'] + extra
    else:
        print(f"Running pipeline (most recent commit on branch {args.branch})")
        check_commit_history()
        run_cmd = ['nextflow', 'run', 'greninger-lab/revica-strm', '-r',  args.branch, '-latest', '--input', 'samplesheet.csv'] + extra

    exc_path = os.path.dirname(__file__)
    print_divider(B_BLUE)

    print_title("RUN BEGIN", B_GREEN)

    try:
        p = subprocess.Popen(run_cmd)
        p.wait()
    except KeyboardInterrupt:
        print("Abort signal recieved! Killing nextflow run...")
        p.send_signal(signal.SIGINT)
        p.send_signal(signal.SIGINT) # sending twice since nextflow wants to confirm
        p.wait()
        sys.exit(1)

    print_divider(B_GREEN)
    sys.exit(0)
